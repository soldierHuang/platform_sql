# docker_airflow.yml
version: '3.8'

services:
  # Airflow Webserver
  airflow-webserver:
    image: benitorhuang/airflow-crawler:0.0.1 
    restart: on-failure
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__WEBSERVER__RBAC: "True"
      # [修正] DAGS_FOLDER 指向專案安裝後的 dags 目錄
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/src/dataflow/dags
      PYTHONPATH: /opt/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True" # 建議新 DAG 默認暫停
      AIRFLOW__CELERY__BROKER_URL: "amqp://${RABBITMQ_DEFAULT_USER:-guest}:${RABBITMQ_DEFAULT_PASS:-guest}@rabbitmq:5672/"
      AIRFLOW__CELERY__RESULT_BACKEND: "db+mysql+pymysql://${MYSQL_USER:-user}:${MYSQL_PASSWORD:-password}@mysql:3306/${MYSQL_DATABASE:-job_data}"
      # [最終修正] PYTHONPATH 只需包含 dags 根目錄，以便找到 'etl' 包
      
    ports:
      - target: 8080
        published: ${AIRFLOW_WEBSERVER_PORT:-8080}
        protocol: tcp
        mode: ingress
    volumes:
      - ${AIRFLOW_LOGS_FOLDER_HOST:-./logs}:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    networks:
      - airflow_internal_net
      - crawler_net
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      replicas: 1

  # Airflow Scheduler
  airflow-scheduler:
    image: benitorhuang/airflow-crawler:0.0.1
    restart: on-failure
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/src/dataflow/dags
      PYTHONPATH: /opt/airflow
      
    volumes:
      - ${AIRFLOW_LOGS_FOLDER_HOST:-./logs}:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    networks:
      - airflow_internal_net
      - crawler_net
    command: scheduler
    deploy:
      replicas: 1

  # Airflow Postgres Database
  airflow-postgres:
    image: postgres:13
    restart: on-failure
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    networks:
      - airflow_internal_net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      replicas: 1

networks:
  airflow_internal_net:
    driver: overlay
    attachable: true
  crawler_net:
    external: true

volumes:
  airflow_postgres_data: